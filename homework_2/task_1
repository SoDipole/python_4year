import re
from pattern.web import Wikipedia, plaintext
from collections import Counter

class WikiParser:
    def __init__(self):
        pass
    
    def get_articles(self, start):
        articles = []
        start_article = Wikipedia().article(start)
        
        for link in start_article.links:
            article = Wikipedia().article(link)
            if article:
                sections = []
                for section in article.sections:
                    text = plaintext(section.source).lower()
                    text = self.parce_text(text)
                    sections.append(text)
                articles.append(" ".join(sections))
        return articles
    
    def parce_text(self, text):
        parced_text = re.sub("\[.+?\]", " ", text)
        parced_text = re.sub("[^\w\s]|\d|\n", " ", parced_text)
        parced_text = re.sub("\s{2,}", " ", parced_text)
        parced_text = re.sub("(^\s)|(\s$)", "", parced_text)
        return parced_text

class TextStatistics:
    def __init__(self, articles):
        self.articles = articles
    
    def get_top_3grams(self, n):
        ngrams = Counter()
        for text in self.articles:
            tokens = text.split()
            ngrams.update([" ".join(tokens[i:i+3]) for i in xrange(len(tokens)-2)])
        ngrams_list = []
        ngrams_freq = []
        for ngram, freq in ngrams.most_common()[:n]:
            ngrams_list.append(ngram)
            ngrams_freq.append(freq)
        return (ngrams_list, ngrams_freq)
    
    def get_top_words(self, n):
        stop_list = ["the", "a", "an", "on", "in", "at", "since", "for", "ago", 
                         "before", "to", "past", "to", "till", "by", "beside", "under", 
                         "below", "over", "above", "across", "through", "into", "towards", 
                         "onto", "from", "of", "off", "until"]        
        words = Counter()
        for text in self.articles:
            tokens = text.split()
            words.update([token for token in tokens if token not in stop_list])
        words_list = []
        words_freq = []
        for word, freq in words.most_common()[:n]:
            words_list.append(word)
            words_freq.append(freq)
        return (words_list, words_freq)
    
class Experiment:
    def __init___(self):
        pass
    
    def show_results(self):
        wp = WikiParser()
        articles = wp.get_articles("Natural language processing")
        
        ts = TextStatistics(articles)
        ngrams_list, ngrams_freq = ts.get_top_3grams(20)
        print("--------------------------\nTop 20 3-grams:\n--------------------------")
        for ngram, freq in zip(ngrams_list, ngrams_freq):
            print(str(ngram) + " : " + str(freq))
            
        words_list, words_freq = ts.get_top_words(20)
        print("--------------------------\nTop 20 words:\n--------------------------")
        for ngram, freq in zip(words_list, words_freq):
            print(str(ngram) + " : " + str(freq)) 
        
        article = Wikipedia().article("Natural language processing")
        sections = []
        for section in article.sections:
            text = plaintext(section.source).lower()
            text = wp.parce_text(text)
            sections.append(text)
        articles = [" ".join(sections)]      
            
        ts_first = TextStatistics(articles)
        ngrams_list, ngrams_freq = ts_first.get_top_3grams(5)
        print("--------------------------\nTop 5 3-grams:\n--------------------------")
        for ngram, freq in zip(ngrams_list, ngrams_freq):
            print(str(ngram) + " : " + str(freq))
            
        words_list, words_freq = ts_first.get_top_words(5)
        print("--------------------------\nTop 5 words:\n--------------------------")
        for ngram, freq in zip(words_list, words_freq):
            print(str(ngram) + " : " + str(freq))         
              
ex = Experiment()
ex.show_results()
"""
--------------------------
Top 20 3-grams:
--------------------------
from the original : 297
archived from the : 287
natural language processing : 276
the use of : 233
the original on : 231
as well as : 224
one of the : 206
a b c : 175
part of speech : 170
the european union : 161
such as the : 151
of the european : 150
cambridge university press : 148
the number of : 147
proceedings of the : 146
a number of : 141
for example the : 139
university press isbn : 135
a set of : 132
based on the : 130
--------------------------
Top 20 words:
--------------------------
and : 16088
is : 8690
as : 5582
that : 4848
are : 4323
or : 3852
language : 3780
s : 3574
with : 3426
be : 3417
it : 2777
this : 2498
which : 2201
can : 1988
not : 1961
i : 1835
p : 1826
also : 1801
was : 1794
english : 1763
--------------------------
Top 5 3-grams:
--------------------------
natural language processing : 13
hand written rules : 6
a chunk of : 6
chunk of text : 6
machine learning algorithms : 5
--------------------------
Top 5 words:
--------------------------
and : 66
language : 57
is : 49
natural : 34
as : 32
"""

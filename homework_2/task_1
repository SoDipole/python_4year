import re
from pattern.web import Wikipedia, plaintext
from collections import Counter

class WikiParser:
    def __init__(self):
        pass
    
    def get_articles(self, start):
        articles = []
        start_article = Wikipedia().article(start)
        
        sections = []
        for section in start_article.sections:
            text = plaintext(section.source).lower()
            text = self.__parce_text(text)
            sections.append(text)
        articles.append(" ".join(sections))
        
        for link in start_article.links:
            article = Wikipedia().article(link)
            if article:
                sections = []
                for section in article.sections:
                    text = plaintext(section.source).lower()
                    text = self.__parce_text(text)
                    sections.append(text)
                articles.append(" ".join(sections))
        return articles
    
    def __parce_text(self, text):
        parced_text = re.sub("\[.+?\]", " ", text)
        parced_text = re.sub("[^\w\s]|\d|\n", " ", parced_text)
        parced_text = re.sub("\s{2,}", " ", parced_text)
        parced_text = re.sub("(^\s)|(\s$)", "", parced_text)
        return parced_text

class TextStatistics:
    def __init__(self, articles):
        self.articles = articles
    
    def get_top_3grams(self, n):
        ngrams = Counter()
        for text in self.articles:
            tokens = text.split()
            ngrams.update([" ".join(tokens[i:i+3]) for i in xrange(len(tokens)-2)])
        ngrams_list = []
        ngrams_freq = []
        for ngram, freq in ngrams.most_common()[:n]:
            ngrams_list.append(ngram)
            ngrams_freq.append(freq)
        return (ngrams_list, ngrams_freq)
    
    def get_top_words(self, n):
        stop_list = ["the", "a", "an", "on", "in", "at", "since", "for", "ago", 
                         "before", "to", "past", "to", "till", "by", "beside", "under", 
                         "below", "over", "above", "across", "through", "into", "towards", 
                         "onto", "from", "of", "off", "until"]        
        words = Counter()
        for text in self.articles:
            tokens = text.split()
            words.update([token for token in tokens if token not in stop_list])
        words_list = []
        words_freq = []
        for word, freq in words.most_common()[:n]:
            words_list.append(word)
            words_freq.append(freq)
        return (words_list, words_freq)
    
class Experiment:
    def __init___(self):
        pass
    
    def show_results(self):
        wp = WikiParser()
        articles = wp.get_articles('Natural language processing')
        
        ts = TextStatistics(articles)
        ngrams_list, ngrams_freq = ts.get_top_3grams(20)
        print("--------------------------\nTop 20 3-grams:\n--------------------------")
        for ngram, freq in zip(ngrams_list, ngrams_freq):
            print(str(ngram) + " : " + str(freq))
            
        words_list, words_freq = ts.get_top_words(20)
        print("--------------------------\nTop 20 words:\n--------------------------")
        for ngram, freq in zip(words_list, words_freq):
            print(str(ngram) + " : " + str(freq)) 
            
        ts_first = TextStatistics([articles[0]])
        ngrams_list, ngrams_freq = ts_first.get_top_3grams(5)
        print("--------------------------\nTop 5 3-grams:\n--------------------------")
        for ngram, freq in zip(ngrams_list, ngrams_freq):
            print(str(ngram) + " : " + str(freq))
            
        words_list, words_freq = ts_first.get_top_words(5)
        print("--------------------------\nTop 5 words:\n--------------------------")
        for ngram, freq in zip(words_list, words_freq):
            print(str(ngram) + " : " + str(freq))
            
            
"""
--------------------------
Top 20 3-grams:
--------------------------
from the original : 297
natural language processing : 289
archived from the : 287
the use of : 234
the original on : 231
as well as : 224
one of the : 208
a b c : 175
part of speech : 173
the european union : 162
such as the : 152
of the european : 150
proceedings of the : 149
cambridge university press : 148
the number of : 148
a number of : 142
for example the : 140
university press isbn : 135
a set of : 134
based on the : 130
--------------------------
Top 20 words:
--------------------------
and : 16154
is : 8739
as : 5614
that : 4872
are : 4343
or : 3876
language : 3837
s : 3587
with : 3441
be : 3433
it : 2785
this : 2510
which : 2221
can : 2002
not : 1971
i : 1838
p : 1827
also : 1810
was : 1800
english : 1768
--------------------------
Top 5 3-grams:
--------------------------
natural language processing : 13
hand written rules : 6
a chunk of : 6
chunk of text : 6
machine learning algorithms : 5
--------------------------
Top 5 words:
--------------------------
and : 66
language : 57
is : 49
natural : 34
as : 32
"""
